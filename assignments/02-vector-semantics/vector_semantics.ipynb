{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Vector Semantics\n",
    "\n",
    "Nikolai Ilinykh, Mehdi Ghanimifard, Wafia Adouane and Simon Dobnik\n",
    "\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will look at how to build distributional semantic models from corpora and use semantic similarity captured by these models to do semantic tasks. We are also going to examine how different vector composition functions for vectors work in approximating semantic similarity of phrases when compared to human judgements.\n",
    "\n",
    "This lab uses code from `dist_erk.py` which contains functions similar to those shown in the lecture. You can use either functions to solve these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following command simply imports all the methods from that code.\n",
    "from dist_erk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a corpus\n",
    "\n",
    "**Important**: All necessary files which are used in this notebook are available on mlt-gpu, check `/srv/data/computational-semantics-assignment-02`.\n",
    "\n",
    "To train a distributional model, we first need a sufficiently large collection of texts which contain different words used frequently enough in different contexts. Here we will use a section of the Wikipedia corpus (`wikipedia.txt`. This file has been borrowed from another lab by [Richard Johansson](http://www.cse.chalmers.se/~richajo/).\n",
    "\n",
    "When unpacked, the file is 151mb, hence if you are using the MLT servers you should store it in a temporary folder outside your home and adjust the `corpus_dir` path below.  \n",
    "<!-- <It may already exist in `/opt/mlt/courses/cl2015/a5`.> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = '/srv/data/computational-semantics-assignment-02/'\n",
    "#corpus_dir = \"computational-semantics-assignment-02/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: small_data: File exists\n",
      "zsh:1: command not found: shuf\n"
     ]
    }
   ],
   "source": [
    "# In case we want to test this on a smaller corpus:\n",
    "!mkdir small_data\n",
    "!shuf -n 100000 /srv/data/computational-semantics-assignment-02/wikipedia.txt > small_data/small_wiki.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dir = './small_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Since the dataset requires a long time to be processed, a small selection will be used instead.\n",
    "In the next section the Pickle module will be used to binarize otherwise big results.\n",
    "\n",
    "Notice that \"small_dir\" will be used instead of the directory containing \"wikipedia.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a model\n",
    "\n",
    "Now you are ready to build the model.  \n",
    "Using the methods from the code imported above build three word matrices with 1000 dimensions as follows:  \n",
    "\n",
    "(i) with raw counts (saved to a variable `space_1k`);  \n",
    "(ii) with PPMI (`ppmispace_1k`);  \n",
    "(iii) with reduced dimensions SVD (`svdspace_1k`).  \n",
    "For the latter use `svddim=5`. **[5 marks]**\n",
    "\n",
    "Your task is to replace `...` with function calls. Functions are imported from `dist_erk.py` earlier, and they largely resemble functions shown during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create count matrices\n",
      "ppmi transform\n",
      "svd transform\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "numdims = 1000\n",
    "svddim = 5\n",
    "\n",
    "# which words to use as targets and context words?\n",
    "# we need to count the words and keep only the N most frequent ones\n",
    "# which function would you use here with which variable?\n",
    "ktw = do_word_count(small_dir,numdims)\n",
    "\n",
    "wi = make_word_index(ktw)\n",
    "words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "# create different spaces (the original matrix space, the ppmi space, the svd space)\n",
    "# which functions with which arguments would you use here?\n",
    "print('create count matrices')\n",
    "space_1k = make_space(small_dir, wi, numdims)\n",
    "print('ppmi transform')\n",
    "ppmispace_1k = ppmi_transform(space_1k, wi)\n",
    "print('svd transform')\n",
    "svdspace_1k = svd_transform(space_1k, numdims,svddim)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: pickles: File exists\n"
     ]
    }
   ],
   "source": [
    "# pickle data:\n",
    "\n",
    "!mkdir pickles\n",
    "import pickle\n",
    "def picklesave(data, name):\n",
    "    filename = 'pickles/'+name+'.pickle'\n",
    "    with open(filename, \"ab\") as outfile:\n",
    "        pickle.dump(data, outfile)\n",
    "\n",
    "picklesave(ktw, 'ktw_1k')\n",
    "picklesave(space_1k, 'space_1k')\n",
    "picklesave(ppmispace_1k, 'ppmispace_1k')\n",
    "picklesave(svdspace_1k, 'svdspace_1k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "Notice that the data is store in pickle format to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import pickled data:\n",
    "\n",
    "with open('pickles/ktw_1k.pickle', 'rb') as file:\n",
    "    ktw = pickle.load(file)\n",
    "with open('pickles/space_1k.pickle', 'rb') as file:\n",
    "    space_1k = pickle.load(file)\n",
    "with open('pickles/ppmispace_1k.pickle', 'rb') as file:\n",
    "   ppmispace_1k = pickle.load(file)\n",
    "with open('pickles/svdspace_1k.pickle', 'rb') as file:    \n",
    "    svdspace_1k = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [41 43 25  8 15  5 11  6  4  1  0  0  2  1  2  2  4  0  2  2  2  0  0  1\n",
      "  0  0  0  2  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  0  0  1  2  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "  0  0  2  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  2  0  0  0  1  0\n",
      "  0  1  0  0  0  0  0  0  0  3  0  0  0  0  0  0  1  0  0  0  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
      "  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  1  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# now, to test the space, you can print vector representation for some words\n",
    "print('house:', space_1k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford Advanced Dictionary has 185,000 words, hence 1,000 words is not representative. We trained a model with 10,000 words, and 50 dimensions on truncated SVD. It took 40 minutes on a laptop. All matrices are available on mlt-gpu: `ktw_wikipediaktw.npy`, `raw_wikipediaktw.npy`, `ppmi_wikipediaktw.npy`, `svd50_wikipedia10k.npy`. Make sure they are in your path, because they will be loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numdims = 10000\n",
    "svddim = 50\n",
    "\n",
    "print('Please wait...')\n",
    "ktw_10k       = np.load(corpus_dir+'ktw_wikipediaktw.npy', allow_pickle=True)\n",
    "space_10k     = np.load(corpus_dir+'raw_wikipediaktw.npy', allow_pickle=True).all()\n",
    "ppmispace_10k = np.load(corpus_dir+'ppmi_wikipediaktw.npy', allow_pickle=True).all()\n",
    "svdspace_10k  = np.load(corpus_dir+'svd50_wikipedia10k.npy', allow_pickle=True).all()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# testing semantic space\n",
    "print('house:', space_10k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Both, 1k vectors and 10k vectors, were correctly visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing semantic similarity\n",
    "\n",
    "The file `similarity_judgements.txt` (a copy is included with this notebook) contains 7,576 pairs of words and their lexical and visual similarities (based on the pictures) collected through crowd-sourcing using Mechanical Turk as described in [1]. The score range from 1 (highly dissimilar) to 5 (highly similar). Note: this is a different dataset from the phrase similarity dataset we discussed during the lecture (the one from [2]). For more information, please read the papers.\n",
    "\n",
    "The following code will transform similarity scores into a Python-friendly format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available words to test: 155\n",
      "number of available word pairs to test: 774\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [] # test suit word pairs\n",
    "semantic_similarity = [] \n",
    "visual_similarity = []\n",
    "test_vocab = set()\n",
    "\n",
    "for index, line in enumerate(open('similarity_judgements.txt')):\n",
    "    data = line.strip().split('\\t')\n",
    "    if index > 0 and len(data) == 3:\n",
    "        w1, w2 = tuple(data[0].split('#'))\n",
    "        # it will check if both words from each pair exist in the word matrix.\n",
    "        if w1 in ktw_10k and w2 in ktw_10k:\n",
    "            word_pairs.append((w1, w2))\n",
    "            test_vocab.update([w1, w2])\n",
    "            semantic_similarity.append(float(data[1]))\n",
    "            visual_similarity.append(float(data[2]))\n",
    "        \n",
    "print('number of available words to test:', len(test_vocab-(test_vocab-set(ktw_10k))))\n",
    "print('number of available word pairs to test:', len(word_pairs))\n",
    "#list(zip(word_pairs, visual_similarity, semantic_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test how the cosine similarity between vectors of each of the three spaces (normal space, ppmi, svd) compares with the human similarity judgements for the words in the similarity dataset. Which of the three spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores, we can use [Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better the similarity scores align. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Here is how you can calculate Pearson's correlation coefficient betweeen the scores of visual similarity and semantic similarity of the available words in the test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Semantic Similarity:\n",
      "rho     = 0.7122\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, visual_similarity)\n",
    "print(\"\"\"Visual Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the cosine similarity scores of all word pairs in an ordered list using all three matrices. **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_similarities  = [cosine(w1, w2, space_10k) for w1, w2 in word_pairs]\n",
    "ppmi_similarities = [cosine(w1, w2, ppmispace_10k) for w1, w2 in word_pairs]\n",
    "svd_similarities  = [cosine(w1, w2, svdspace_10k) for w1, w2 in word_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "From the \"dist_erk.py\" file, the cosine function is used to compare words from different spaces in a very convenient way by allowing the \"space\" argument.\n",
    "In the following code block a comparison is given as a means for quick visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pair: ('stick', 'sword')\t\t Raw:     82.25\t PPMI:   7.67\t SVD:    73.60\n",
      " Pair: ('cabin', 'cabinet')\t\t Raw:     88.72\t PPMI:   3.67\t SVD:    39.83\n",
      " Pair: ('chicken', 'sparrow')\t\t Raw:     81.86\t PPMI:   2.61\t SVD:    37.22\n",
      " Pair: ('bag', 'gate')\t\t Raw:     86.41\t PPMI:   6.01\t SVD:    59.76\n",
      " Pair: ('bull', 'trumpet')\t\t Raw:     92.09\t PPMI:   3.01\t SVD:    36.90\n",
      " Pair: ('balloon', 'brick')\t\t Raw:     83.66\t PPMI:   5.53\t SVD:    58.93\n",
      " Pair: ('helicopter', 'jet')\t\t Raw:     91.06\t PPMI:  14.65\t SVD:    91.94\n",
      " Pair: ('cape', 'spider')\t\t Raw:     83.96\t PPMI:   4.53\t SVD:    49.44\n",
      " Pair: ('boots', 'mouse')\t\t Raw:     75.89\t PPMI:   4.95\t SVD:    55.08\n",
      " Pair: ('kite', 'marble')\t\t Raw:     87.30\t PPMI:   3.43\t SVD:    44.88\n",
      " Pair: ('box', 'horse')\t\t Raw:     91.96\t PPMI:   8.51\t SVD:    74.80\n",
      " Pair: ('doll', 'elephant')\t\t Raw:     78.23\t PPMI:   5.28\t SVD:    79.26\n",
      " Pair: ('brick', 'cannon')\t\t Raw:     93.86\t PPMI:   9.03\t SVD:    67.66\n",
      " Pair: ('ruler', 'turkey')\t\t Raw:     67.13\t PPMI:  11.63\t SVD:    60.67\n",
      " Pair: ('pan', 'pot')\t\t Raw:     81.20\t PPMI:   7.36\t SVD:    58.58\n",
      " Pair: ('bottle', 'orange')\t\t Raw:     77.62\t PPMI:   4.89\t SVD:    69.32\n",
      " Pair: ('plate', 'rock')\t\t Raw:     87.58\t PPMI:   7.37\t SVD:    45.22\n",
      " Pair: ('subway', 'wall')\t\t Raw:     83.34\t PPMI:   7.24\t SVD:    52.09\n",
      " Pair: ('cup', 'level')\t\t Raw:     84.98\t PPMI:   7.88\t SVD:    32.94\n",
      " Pair: ('book', 'submarine')\t\t Raw:     80.10\t PPMI:   5.77\t SVD:    24.45\n"
     ]
    }
   ],
   "source": [
    "# The following is a quick visualization for some pairs of words across different spaces.\n",
    "\n",
    "pairs = [(w1, w2) for w1, w2 in word_pairs]\n",
    "for pair, raw_similary, ppmi_similarity, svd_similarity in zip(pairs[:20], raw_similarities[:20], ppmi_similarities[:20], svd_similarities[:20]):\n",
    "    print(f\" Pair: {pair}\\t\\t Raw: {100*raw_similary:9.2f}\\t PPMI: {100*ppmi_similarity:6.2f}\\t SVD: {100*svd_similarity:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "It can be easily observed how the three models present conflicting results: comparissons such as \"helicopter\" and \"jet\" score highly in all three, but \"doll\" and \"elephant\" score lowly in PPMI and highly in the other two models, others such as \"book\" and \"submarine\" score highly in the raw space similarity, but lowly in the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate correlation coefficients between lists of similarity scores and the real semantic similarity scores from the experiment. The scores of what model best correlates them? Is this expected? **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Similarity vs. Semantic Similarity:\n",
      "rho     = 0.1522\n",
      "p-value = 0.0000\n",
      "\n",
      "\n",
      "PPMI Similarity vs. Semantic Similarity:\n",
      "rho     = 0.4547\n",
      "p-value = 0.0000\n",
      "\n",
      "\n",
      "SVD Similarity vs. Semantic Similarity:\n",
      "rho     = 0.4232\n",
      "p-value = 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rho, pval = stats.spearmanr(raw_similarities, semantic_similarity)\n",
    "print(\"\"\"Raw Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "rho, pval = stats.spearmanr(ppmi_similarities, semantic_similarity)\n",
    "print(\"\"\"PPMI Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "rho, pval = stats.spearmanr(svd_similarities, semantic_similarity)\n",
    "print(\"\"\"SVD Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the comparison provided above we can conclude that PPMI approximates, by a slight margin, best to human judgements. However, both PPMI and SVD are rather distant from what is expected.\n",
    "SVD, in its simplicity may lose part of important context, but that loss is, probably, not significant enough (with the dataset that we work with). PPMI presents a good probabilistic approach, which is much better that just co-occurrences in the \"raw\" similarities.\n",
    "This reasoning corresponds with the fact that more sophisticated probabilistic approaches work better than just the mere surrounding context of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate correlation coefficients between lists of cosine similarity scores and the real visual similarity scores from the experiment. Which similarity model best correlates with them? How do the correlation coefficients compare with those from the previous comparison - and can you speculate why do we get such results? **[7 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Similarity vs. Visual Similarity:\n",
      "rho     = 0.1212\n",
      "p-value = 0.0007\n",
      "\n",
      "\n",
      "PPMI Similarity vs. Visual Similarity:\n",
      "rho     = 0.3838\n",
      "p-value = 0.0000\n",
      "\n",
      "\n",
      "SVD Similarity vs. Visual Similarity:\n",
      "rho     = 0.3097\n",
      "p-value = 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rho, pval = stats.spearmanr(raw_similarities, visual_similarity)\n",
    "print(\"\"\"Raw Similarity vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "print(\"\\n\")\n",
    "\n",
    "rho, pval = stats.spearmanr(ppmi_similarities, visual_similarity)\n",
    "print(\"\"\"PPMI Similarity vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "print(\"\\n\")\n",
    "\n",
    "rho, pval = stats.spearmanr(svd_similarities, visual_similarity)\n",
    "print(\"\"\"SVD Similarity vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the visual similarity are consistent with those of the semantic similarity, but in this case PPMI does  even better than SVD and \"Raw\" similarity does slightly worse.\n",
    "Judging by the first cosine similarity comparison between Visual Similarity and Semantic Similarity, it being around 0.7 already hints that the results of the operations performed to them, no matter which, should't be too different from each other. This is proved above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform mathematical operations on vectors to derive meaning predictions.\n",
    "\n",
    "For example, we can perform `king - man` and add the resulting vector to `woman` and we hope to get the vector for `queen`. Also, what would be the result of `stockholm - sweden + denmark`? Why? **[3 marks]**\n",
    "\n",
    "If you want to learn more about vector differences between words (and words in analogy relations), check [this paper](https://aclanthology.org/P16-1158.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute \"king - man + woman = queen\" we are retaining the unique characteristics of the first semantic element and applying them to the third semantic element. In this case we could say the chracteristics of that first element are equivalent to \"royalty\"(N) or \"regal\"(ADJ) and, since those are applied to a person (man), altering the \"polarity\" of such  element gives us the opposite gender of the \"regal entity\", that is \"queen\".\n",
    "If the same process is applied to \"stockholm - sweden + denmark\" we are left with the characteristic \"city\", a subset of the entities that make up a \"country\". Is interesting how we, then, automatically take into account the most salient relation of \"Stockholm\", a city, to the following element \"Sweden\", a country, which is \"capital\". Thus, we are left with the characteristics \"city\" and \"capital\" to \"Denmark\", which result in no other entity than \"Copenhagen\".\n",
    "In the first example there is a relation inseparable from polarity: there's a King and a Queen, and because of this, the relation might be much more evident or may come to mind much more easily than in the second example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some helpful code that allows us to calculate such comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(vec):\n",
    "    return vec / veclen(vec)\n",
    "\n",
    "def find_similar_to(vec1, space):\n",
    "    # vector similarity funciton\n",
    "    #sim_fn = lambda a, b: 1-distance.euclidean(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.correlation(a, b)\n",
    "    #sim_fn = lambda a, b: 1-distance.cityblock(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.chebyshev(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: np.dot(normalize(a), normalize(b))\n",
    "    sim_fn = lambda a, b: 1-distance.cosine(a, b)\n",
    "\n",
    "    sims = [\n",
    "        (word2, sim_fn(vec1, space[word2]))\n",
    "        for word2 in space.keys()\n",
    "    ]\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you apply this code. Comment on the results you get. **[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('long', 0.8733111261346901), ('above', 0.8259671977311955), ('around', 0.8030776291120685), ('sun', 0.7692439111243973), ('just', 0.7678481974778111), ('wide', 0.767257431992253), ('each', 0.7665960260861158), ('circle', 0.7647746702909336), ('length', 0.7601066921319761), ('almost', 0.7542351860536628)]\n",
      "[('short', 0.8352373661313067), ('lengthy', 0.7727685249956753), ('occasional', 0.7581408627401973), ('brief', 0.7532022457352894), ('opening', 0.7125803524384261), ('subsequent', 0.7062952429131073), ('previous', 0.7037420867207232), ('earlier', 0.7013170065105806), ('dramatic', 0.693981584503108), ('included', 0.679120407810342)]\n",
      "[('heavy', 0.859207118765945), ('tank', 0.7378450414660778), ('fire', 0.7359730012834718), ('gun', 0.7301315160311267), ('ground', 0.725716751109639), ('air', 0.7226095580472032), ('light', 0.7147705210063771), ('combat', 0.7028027559677159), ('shells', 0.7026242122106345), ('landing', 0.698614491946687)]\n",
      "[('heavy', 0.859207118765945), ('tank', 0.7378450414660778), ('fire', 0.7359730012834718), ('gun', 0.7301315160311267), ('ground', 0.725716751109639), ('air', 0.7226095580472032), ('light', 0.7147705210063771), ('combat', 0.7028027559677159), ('shells', 0.7026242122106345), ('landing', 0.698614491946687)]\n"
     ]
    }
   ],
   "source": [
    "short = normalize(svdspace_10k['short'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "long = normalize(svdspace_10k['long'])\n",
    "heavy = normalize(svdspace_10k['heavy'])\n",
    "\n",
    "print(find_similar_to(light - (heavy - long), svdspace_10k)[:10])\n",
    "print(find_similar_to((heavy - light) + short, svdspace_10k)[:10])\n",
    "print(find_similar_to((heavy + long) - short, svdspace_10k)[:10])\n",
    "print(find_similar_to((heavy - short) + long, svdspace_10k)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By testing a pair of analogies we can confirm that categories that may come easily to our mind are not computed in a similar way by vector operations. Relations such as \"heavy and light\" or \"long and short\" are not captured very precisely.\n",
    "\n",
    "Looking at a computations such as \"light - (heavy - long)\", the binary relation should jump to mind very easily with \"short\" as the answer. Let's think about the characteristics of the semantics elements:\n",
    "\"light\" implies \"poor weight\", \"heavy\" implies \"great weight\" and \"long\" implies \"great length\", subtracting \"great weight\" from \"great length\" does not result in any precise meaning, but leaves us with an equivalence rule: there are two implicit entities that entail polarity, \"weight\" and \"length\". Their value, great or poor, becomes relevant after taking into account the third element \"light\", which reveals \"poor weight\" to the polarity equivalence. That is probably why \"poor length\" comes to mind almost inmediately, it is the X missing in the correlation.\n",
    "\n",
    "All this, however, can only be done when the pure semantics of the element is understood. A distributional approach is, by definition, looking outside the element, and not inside.\n",
    "\n",
    "The results above capture one of the words themselves, synonyms or words that may describe an object or entity with the characteristics used in the correlations. As discussed previously, this is a result of the observation of the word environment (most likely).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 5 similar pairs of pairs of words and test them. Hint: Google for `word analogies examples`. You can also construct analogies that are less lexical but more grammatical, e.g. `see, saw, leave, ?` or analogies that are based on world knowledge as in the [Google analogy dataset](http://download.tensorflow.org/data/questions-words.txt) from [3]. Does the resulting vector similarity confirm your expectations? But remember you can only do this if the words are contained in our vector space with 10,000 dimensions. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"go\" is to \"went\" as \"see\" is to...\n",
      "[('say', 0.7628181643929222), ('find', 0.7437847252209391), ('compare', 0.7431958450277302), ('follow', 0.7417741118308697), ('see', 0.7388715813993888), ('understand', 0.7326518065260651), ('make', 0.7312414374627294), ('add', 0.7250016596149815), ('go', 0.7210654110531284), ('know', 0.7169942587586754)]\n",
      "\"sea\" is to \"water\" as \"desert\" is to...\n",
      "[('desert', 0.8631367536147341), ('mediterranean', 0.8065760895275305), ('sea', 0.805695219711723), ('mountains', 0.790307080574741), ('coastal', 0.7892410225619448), ('arctic', 0.7867701788482583), ('ocean', 0.7834812454928333), ('gulf', 0.7754828475869852), ('seas', 0.7748867641663814), ('plains', 0.7722229522760461)]\n",
      "\"foot\" is to \"feet\" as \"table\" is to...\n",
      "[('table', 0.9173713911491298), ('ring', 0.8314758899238498), ('anchor', 0.8217527498343041), ('pool', 0.8143377247211394), ('chain', 0.8109512852776265), ('floating', 0.8092960776279715), ('window', 0.8089380556210312), ('box', 0.8041957043663301), ('pin', 0.7960496686737227), ('tables', 0.7959952357625978)]\n",
      "\"love\" is to \"hate\" as \"night\" is to...\n",
      "[('night', 0.8770874498772704), ('day', 0.7620968201489743), ('episode', 0.7542667135783017), ('movie', 0.7542595473524939), ('show', 0.7473663076558447), ('boy', 0.7396503672262972), ('star', 0.7389223819697196), ('song', 0.732979538148823), ('dawn', 0.7312367965905464), ('christmas', 0.7151568378911332)]\n",
      "\"man\" is to \"programmer\" as \"woman\" is to...\n",
      "[('man', 0.8644499467474603), ('woman', 0.8374068216530351), ('girl', 0.7802955357360608), ('love', 0.7526478634137278), ('mother', 0.7367947667852422), ('child', 0.7263396926665124), ('boy', 0.7256216788185225), ('her', 0.7204453426518094), ('baby', 0.7060890226325613), ('my', 0.6991971101564293)]\n"
     ]
    }
   ],
   "source": [
    "analogies = [('go', 'went', 'see', 'saw'), \n",
    "             ('sea', 'water', 'desert', 'sand'), \n",
    "             ('foot', 'feet', 'table', 'tables'), \n",
    "             ('love', 'hate', 'night', 'day'), \n",
    "             ('man', 'programmer', 'woman', 'programmer')\n",
    "            ]\n",
    "\n",
    "for analogy in analogies:\n",
    "    a,b,c,d = analogy\n",
    "    print('\"{}\" is to \"{}\" as \"{}\" is to...'.format(a, b, c))\n",
    "    print(find_similar_to(normalize(svdspace_10k[a]) - (normalize(svdspace_10k[b]) - normalize(svdspace_10k[c])), svdspace_10k)[:10])\n",
    "    if d not in svdspace_10k:\n",
    "        print(d, 'not in vector space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Analogies of the type described earlied ('sea' to 'water' as 'desert' to 'sand') are, as already discussed, not captured in the vectors, but the same goes for grammatical or paradigmatic relations: ('foot', 'feet', 'table', 'tables') or ('go', 'went', 'see', 'saw'), singular-plural or verb paradigms are not taken into account. \n",
    "Other relations such as ('man', 'programmer', 'woman', 'programmer'), not as cleary defined as \"king and queen\", also fail our expectations.\n",
    "From our set, the only analogy that approaches our understanding is ('love', 'hate', 'night', 'day'), with the relation \"night\" being \"night\", the same word, in first place (which is itself a rather odd thing to suggest), but \"day\" as second. This is a comparatively good result, but we may venture that the result is a mere coincidence: the statistics may come from expressions such as \"the difference is like night and day\" being frequent in the language. \n",
    "This particular case shows that context may be useful to get our desired interpretation of a meaning, but, again, as seen in multiple occasions through this exercise, it is void of the specific semantics pertaining to that word. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic composition and phrase similarity **[20 marks]**\n",
    "\n",
    "In this task, we are going to look at how different semantic composition models, introduced in [2] correlate with human judgements. The file with the dataset `mitchell_lapata_acl08.txt` is included with this notebook (we also used it in the class).\n",
    "\n",
    "---\n",
    "\n",
    "Explanation of the task from Discord channel:\n",
    "\n",
    "**What are we trying to achieve?**  \n",
    "We want to create models, which can automatically capture differences between meaning of different phrases. These models should also be as good as we are (humans) in this task. Check example (1) from Mitchell/Lapata paper, it has two sentences which share the same words, but their meaning is completely different. We as humans can clearly see this difference, but how could a machine capture it?\n",
    "\n",
    "It is intuitive that in order to get a meaning of a phrase, we might combine meaning of individual words in this phrase somehow, but how? First, we represent each word with the frequency vector based on the semantic space that we built before (e.g, frequency space, ppmi space, svd space). In other words, each word's meaning is represented by the number of times other words occur in the context, defined by window size. Now, we have a vector for `discussion` and a vector for `thrive`.\n",
    "\n",
    "_How do we combine these vectors to get a single vector for the phrase `discussion thrive`?_\n",
    "\n",
    "Such methods of combining meaning vectors into a single item are called _semantic composition methods_ (literally, because we compose semantic meaning of the phrase from its individuals). During the lecture, we tried different semantic composition methods: additive, multiplicative, combined.\n",
    "\n",
    "Let's say we multiplied the vectors (went with the multiplicative method) and now we have one vector for our phrase.\n",
    "\n",
    "Remember, we want to have a model that captures differences between the phrases; it means that if `discussion thrive` is our reference phrase, we need to have a different phrase (high or low similarity phrase) to compare it against the reference one. How do we get this other phrase? Well, this other phrase can be either very similar to the reference or not similar at all, right? Let's say we decided to go with the second option and made/constructed a phrase `discussion digress`, which we know is very dissimilar to the reference phrase. We label this pair of phrases as having a low similarity, e.g., `low` in `hilo`. We can also create a different phrase (e.g., `discussion develop`) and use it as a high similarity phrase when paired with our reference phrase, right? This then would be labeled as `high` in `hilo`. This is what `hilo` in the dataset stands for: known information about how similar the reference phrase and the landmark phrase are.\n",
    "\n",
    "Now, our main task is to automatically learn the similarities/differences between our reference and our landmark, right? We take the first pair: `discussion thrive` vs. `discussion digress`. We have a vector representation for each of these phrases.\n",
    "\n",
    "**How do we compare two vectors?**\n",
    "\n",
    "We use cosine similarity to calculate a single score that would tell us about the similarity between these two vectors. The bigger the cosine, the more similar two vectors are. Cosine ranges from 0 to 1 (0 is very low similarity, 1 is very high similarity). Let's say, we get a cosine of 0.89, it means that according to the multiplicative model (remember, we decided to use multiplicative semantic composition method), these phrases are very similar (cosine is quite high). But wait a second, we know that these two phrases should be of low similarity, right? Because this is what the value in `hilo` tells us about this pair - `discussion thrive` and `discussion digress` are not similar to each other. Clearly, our multiplicative method fails to capture it.\n",
    "\n",
    "**What can we do to improve our model?**\n",
    "\n",
    "We can try a different composition method to get a phrase vector from phrase's words: let's replace multiplication with addition. Or we can also use combined method. Let's say we used combined method and run cosine again; this time it tells that the cosine score is 0.45. Ok, this seems to be quite low, and it also agrees with our knowledge that these phrases are indeed not similar.\n",
    "\n",
    "---\n",
    "\n",
    "In other words, we need to evaluate different composition models (additive, multiplicative, combined) and analyse how well they perform. **How do we analyse their performance?** Because we have the ground-truth for comparison (`hilo` values), we know whether our phrases are actually similar or not. We want our cosine score to reflect this knowledge: if the score is high, but the groun-truth hilo is low, then we have a problem in the model - it did not learn things well, we need to replace the composition function.\n",
    "\n",
    "---\n",
    "\n",
    "_Long story short_: `hilo` is something that we compare our cosine to. `hilo` contains correct answers about similarity between phrases, and cosine should agree with this. If reference-landmark pair are `high` in `hilo`, then cosine should be high enough to reflect that. If cosine is not high in this case, then we look at our model and change the composition function. We need to find the function, which give us cosines that are super close to the ground-truth known `hilo` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compute do we compute correlation between our model's predictions and the ground-truth, something similar to the results from [2] (image above):\n",
    "\n",
    "---\n",
    "\n",
    "In `High` and `Low` columns we have mean cosine values.\n",
    "These are calculated by averaging cosine scores for all pairs of phrases per model.\n",
    "Rows introduce different models: `add` is additive, `multiply` is multiplicative, etc. (these are all described in the paper). `NonComp` is a baseline model, the most \"stupid\" one, it should be the worst. `UpperBound` is how humans performed in this task (they were asked to rate similarity between pairs of phrases, 1 is the lowest, 7 is the highest). Why these numbers are in a different scale, not from 0 to 1 like cosine, but from 1 to 7? Because they are not normalised, and authors explicitly said that they are interested in relative differences.\n",
    "\n",
    "We need models which are closer to human ratings. `Add` model has 0.59 mean for `High` and 0.59 for `Low`, so it did not learn to differentiate between high similarity pairs and low similarity ones. This is a bad model then, we need a better one. `WeightAdd` seems to be doing better, the difference between `High` and `Low` is now 0.01, but it's also quite bad - the difference is not that obvious. The best models are `Multiply` and `Combined`, because their mean cosines for `High` and `Low` are quite different from each other. We can see that these two models gave higher cosines for high similarity pairs (0.42 and 0.38), while giving lower cosines for low similarity pairs (0.28 and 0.28). And this is a good result - it shows that these two composition functions are so far the best in (i) giving high cosine go highly similar pairs and low cosine to very dissimilar pairs, and (ii) keeping the distance (range) between high and low cosines quite large.\n",
    "\n",
    "**However, we can't say how far/close they are when compared to human performance (UpperBound) since human scores are not normalised.**\n",
    "\n",
    "Still, we want to choose a model which is the closest to humans. This is why we want to run **the correlation test.**\n",
    "\n",
    "How do we perform the correlation test? We need to see how well *each model* correlates with human judgements. So for each model, we would have a vector of cosine values this model gives for each pair that we have. For example, let's say we have three pairs of phrases and our cosine values from additive model are the following ones: `[0.89, 0.40, 0.70]`. Now we need to get a vector of the same size, but for human scores. What do we have for human scores? We have multiple participants, which means that a single phrase can be evaluated by multiple participants. Let's say, the first phrase has scores from two participants (`6, 7`), so what we would do is that we would average it to have a single number (`(6 + 7) / 2 = 6.5`). With this, we can get a mean vector of human scores per item: `[6.5, 3, 6]`.\n",
    "\n",
    "Now, we have two vectors and we can run Spearman correlation on these vectors. This is what exactly what the third column in the Results table is showing (the correlation value). There is also a p-value (denoted right below the table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is your ultimate task in this part of the assignment?**\n",
    "\n",
    "(i) Process the dataset and extract `reference - landmark` pairs; you can use the code from the lecture as something to start with. Try to keep information about human rating (`input`) and high/low similarity (`hilo`), because you will need it for correlation tests. Also, you might want to keep the information about participant id (will be useful for getting average numbers for correlation tests). Which format you should use to keep all this data? It's up to you, but a dictionary-like format could be a good choice.\n",
    "\n",
    "(ii) Build models of semantic phrase composition: in the lecture we introduced simple additive, simple multiplicative and combined models (details are in [2]). Your task is to take a single pair of phrases, and compute the composition of its vectors using each of these functions. Thus, you will have (at least) three compositional models that take each `noun - verb` phrase from the pair (these phrases can be either references or landmarks) and output a single vector, representing the meaning of this phrase. As your semantic space, you can use pretrained spaces (standard space, ppmi or svd) introduced above. It is up to you which space you use, but for someone who runs your code, it should be pretty straightforward to switch between them.\n",
    "\n",
    "(iii) calculate Spearman correlation between each model's predictions and human judgements; you should have something similar to the scores that are shown in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts process behind calculating the correlation:**\n",
    "\n",
    "Let's look at the example pair: reference `child chatter` and high-level similarity landmark (as the last word in the row indicates) `child gabble`. Let's say we have 3 humans evaluating the similarity between these two phrases and we combine their scores into a single vector: `[5, 6, 5]`. We need to average them to get our human vector for correlation: `[5.3]`.\n",
    "\n",
    "Our A model's output:  \n",
    "`cosine(p1, p2) = 0.88`, where p1 is the result of addition of word vectors in the reference phrase `child gabble`, and p2 is the result of addition of word vectors in the high-level similarity phrase `child chatter`.  \n",
    "\n",
    "Therefore, we have human rating vector `[5.3]` and model A output `[0.88]`. Next is to compute correlation between these two vectors. This should give you a correlation value and p-value for the model of choice and human ratings.\n",
    "\n",
    "Of course, your human rating vectors will be longer (e.g., [6, 7, 3, 4, 5]). Each of your models (A, B, C) will produce a single vector of cosine similarity between these same pairs (e.g., [0.89, 0.98, 0.23, 0.65, 0.55]). The goal is to compare each model's cosine similarity vectors with human rating vectors and identify the model which outputs the best result in terms of being the closest to the way human rate similarity between the phrases.\n",
    "\n",
    "---\n",
    "\n",
    "**The minimum to do in this task**: compute correlations for at least _ONE_ model and human ratings. However, this should not be hard to run it for any other model as well. For examples on how to interpret the results, look at Section 5 Results of the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"mitchell_lapata_acl08.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbeModels:\n",
    "    def __init__(self, filename, space, model, \n",
    "                 alpha = 0, beta = 0.95, gamma = 0.05,\n",
    "                 weight_n = 0.2, weight_v=0.8):\n",
    "        self.df = pd.read_csv(filename, sep=\" \")\n",
    "        self.space = space\n",
    "        self.model = model\n",
    "        self.alpha = alpha \n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.weight_n = weight_n\n",
    "        self.weight_v = weight_v\n",
    "        \n",
    "    def combine_vectors(self, vector1, vector2):\n",
    "        if self.model == \"multiply\":\n",
    "            return vector1 * vector2\n",
    "        elif self.model == \"add\":\n",
    "            return vector1 + vector2\n",
    "        elif self.model == \"weighted_add\":\n",
    "            return self.weight_n * vector1 + self.weight_v * vector2\n",
    "        elif self.model == \"combine\":\n",
    "            return self.alpha * vector1 + self.beta * vector2 + self.gamma * vector1 * vector2\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.model} is not implemented\")\n",
    "    \n",
    "    def get_similarities(self, vec1, vec2):\n",
    "        veclen1 = veclen(vec1)\n",
    "        veclen2 = veclen(vec2)\n",
    "\n",
    "        if veclen1 == 0.0 or veclen2 == 0.0:\n",
    "            # one of the vectors is empty. make the cosine zero.\n",
    "            return 0.0\n",
    "\n",
    "        else:\n",
    "            # we could also simply do:\n",
    "            # dotproduct = numpy.dot(vec1, vec2)\n",
    "            dotproduct = numpy.sum(vec1 * vec2)\n",
    "\n",
    "            return dotproduct / (veclen1 * veclen2)\n",
    "        \n",
    "    def get_spearman(self, sims, human):\n",
    "        rho, pval = stats.spearmanr(sims, human)\n",
    "        return rho, pval\n",
    "        \n",
    "    def process_row(self, row):\n",
    "        if row[\"noun\"] in self.space and row[\"verb\"] in self.space and row[\"landmark\"] in self.space:\n",
    "            noun_vector = self.space[row[\"noun\"]]\n",
    "            verb_vector = self.space[row[\"verb\"]]\n",
    "            land_vector = self.space[row[\"landmark\"]]\n",
    "\n",
    "            ref_vector = self.combine_vectors(noun_vector, verb_vector)\n",
    "            land_vector = self.combine_vectors(noun_vector, land_vector)\n",
    "\n",
    "            cos_sim = self.get_similarities(ref_vector, land_vector)\n",
    "            return cos_sim.tolist()\n",
    "        \n",
    "    def get_human_score(self):\n",
    "        unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
    "        high_mean = unique_triplets[unique_triplets[\"hilo\"]==\"high\"][\"input\"].mean()\n",
    "        low_mean = unique_triplets[unique_triplets[\"hilo\"]==\"low\"][\"input\"].mean()\n",
    "        return {\"model\": \"human\", \"High\": high_mean, \"Low\": low_mean, \"R\": 1.0, \"p-value\": 0.0}\n",
    "    \n",
    "    def probe(self):\n",
    "        unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
    "        unique_triplets[\"sims\"] = unique_triplets.apply(self.process_row, axis=1)\n",
    "        unique_triplets = unique_triplets[unique_triplets.sims.notna()]\n",
    "        r, p = self.get_spearman(unique_triplets[\"sims\"], unique_triplets[\"input\"])\n",
    "        high_mean = unique_triplets[unique_triplets[\"hilo\"]==\"high\"][\"sims\"].mean()\n",
    "        low_mean = unique_triplets[unique_triplets[\"hilo\"]==\"low\"][\"sims\"].mean()\n",
    "        return {\"model\": self.model, \"High\": high_mean, \"Low\": low_mean, \"R\": r, \"p-value\": p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(filename, space):\n",
    "    multiply = ProbeModels(filename, space, \"multiply\")\n",
    "    combined = ProbeModels(filename, space, \"combine\")\n",
    "    add = ProbeModels(filename, space, \"add\")\n",
    "    weighted_add = ProbeModels(filename, space, \"weighted_add\")\n",
    "    \n",
    "    scores = []\n",
    "    scores.append(multiply.get_human_score())\n",
    "    scores.append(multiply.probe())\n",
    "    scores.append(add.probe())\n",
    "    scores.append(weighted_add.probe())\n",
    "    scores.append(combined.probe())\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for raw space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3436275/216900176.py:59: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>R</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>5.084540</td>\n",
       "      <td>3.286199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multiply</td>\n",
       "      <td>0.908243</td>\n",
       "      <td>0.895953</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.693239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add</td>\n",
       "      <td>0.986482</td>\n",
       "      <td>0.964184</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.260405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_add</td>\n",
       "      <td>0.941778</td>\n",
       "      <td>0.861051</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.319889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combine</td>\n",
       "      <td>0.902514</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.492726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model      High       Low         R   p-value\n",
       "0         human  5.084540  3.286199  1.000000  0.000000\n",
       "1      multiply  0.908243  0.895953  0.166667  0.693239\n",
       "2           add  0.986482  0.964184  0.452381  0.260405\n",
       "3  weighted_add  0.941778  0.861051  0.404762  0.319889\n",
       "4       combine  0.902514  0.879121  0.285714  0.492726"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Results for raw space\")\n",
    "run_all_models(filename, space_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ppmi space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3436275/216900176.py:59: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>R</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>5.084540</td>\n",
       "      <td>3.286199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multiply</td>\n",
       "      <td>0.077045</td>\n",
       "      <td>0.035370</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.138960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add</td>\n",
       "      <td>0.629155</td>\n",
       "      <td>0.558364</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.085559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_add</td>\n",
       "      <td>0.157083</td>\n",
       "      <td>0.130878</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.057990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combine</td>\n",
       "      <td>0.053801</td>\n",
       "      <td>0.052853</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.351813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model      High       Low         R   p-value\n",
       "0         human  5.084540  3.286199  1.000000  0.000000\n",
       "1      multiply  0.077045  0.035370  0.571429  0.138960\n",
       "2           add  0.629155  0.558364  0.642857  0.085559\n",
       "3  weighted_add  0.157083  0.130878  0.690476  0.057990\n",
       "4       combine  0.053801  0.052853  0.380952  0.351813"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Results for ppmi space\")\n",
    "run_all_models(filename, ppmispace_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for svd space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3436275/216900176.py:59: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n",
      "/tmp/ipykernel_3436275/216900176.py:65: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  unique_triplets = self.df.groupby([\"noun\", \"verb\", \"landmark\", \"hilo\"]).mean().reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>R</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>5.084540</td>\n",
       "      <td>3.286199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multiply</td>\n",
       "      <td>0.934981</td>\n",
       "      <td>0.931578</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.910849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add</td>\n",
       "      <td>0.926754</td>\n",
       "      <td>0.876599</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.138960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_add</td>\n",
       "      <td>0.702953</td>\n",
       "      <td>0.644893</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.319889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combine</td>\n",
       "      <td>0.251353</td>\n",
       "      <td>0.293039</td>\n",
       "      <td>-0.095238</td>\n",
       "      <td>0.822505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model      High       Low         R   p-value\n",
       "0         human  5.084540  3.286199  1.000000  0.000000\n",
       "1      multiply  0.934981  0.931578  0.047619  0.910849\n",
       "2           add  0.926754  0.876599  0.571429  0.138960\n",
       "3  weighted_add  0.702953  0.644893  0.404762  0.319889\n",
       "4       combine  0.251353  0.293039 -0.095238  0.822505"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Results for svd space\")\n",
    "run_all_models(filename, svdspace_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any comments/thoughts should go here:**\n",
    "\n",
    "These results do not agree with the results from the paper. On the other hand, a lot of the example sentence pairs could not be included because they contain out of vocabulary words, so even if the results had agreed with those in the paper, they should still be taken with a (large) grain of salt. As we say in these cases: it is not our fault, it's the data.\n",
    "\n",
    "Even so, it is interesting that the versions that showed the highest correlation in the paper showed a very low correlation here (even negative in the case of the combined model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "[1] C. Silberer and M. Lapata. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721732, Baltimore, Maryland, USA, June 2325 2014 2014. Association for Computational Linguistics.  \n",
    "\n",
    "[2] Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236244). Association for Computational Linguistics.\n",
    "  \n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 31113119, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 60 marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
